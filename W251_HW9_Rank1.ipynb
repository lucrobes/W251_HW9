{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00e8efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1eb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Download Imagenet train Dataset\"\"\"\n",
    "\n",
    "def download_data():\n",
    "\n",
    "    for i in os.listdir():\n",
    "        \n",
    "        if '.tar' in i:\n",
    "            \n",
    "            os.remove(i)\n",
    "    \n",
    "    !wget https://w251hw05.s3.us-west-1.amazonaws.com/ILSVRC2012_img_train.tar\n",
    "\n",
    "    #!wget https://w251hw05.s3.us-west-1.amazonaws.com/ILSVRC2012_img_val.tar\n",
    "    \n",
    "    print(os.listdir())\n",
    "\n",
    "#download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f712907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "    \n",
    "    import tarfile \n",
    "\n",
    "    if os.path.exists('Train'):\n",
    "        \n",
    "        print('extracting training data only')\n",
    "        \n",
    "        \"\"\"Extract all .Tar Files in Training Data\"\"\"\n",
    "    \n",
    "        for i in range(len(os.listdir('Train'))):\n",
    "\n",
    "            if '.tar' in os.listdir('Train')[i]:\n",
    "\n",
    "                tftn = tarfile.open(f\"Train/{os.listdir('Train')[i]}\")\n",
    "\n",
    "                tftn.extractall(f'Train/{i}')\n",
    "\n",
    "                tftn.close()\n",
    "\n",
    "                print(i, len(os.listdir(f'Train/{i}')))\n",
    "\n",
    "\n",
    "        \"\"\"Remove all .tar files from directory\"\"\"\n",
    "\n",
    "\n",
    "        for i in os.listdir('Train'):\n",
    "\n",
    "            if '.tar' in i: \n",
    "\n",
    "                os.remove(f'Train/{i}')\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        tft = tarfile.open('ILSVRC2012_img_train.tar')\n",
    "\n",
    "        tft.extractall('Train')\n",
    "\n",
    "        tft.close()\n",
    "\n",
    "        if os.path.exists('ILSVRC2012_img_val.tar'):\n",
    "\n",
    "            tfv = tarfile.open('ILSVRC2012_img_val.tar')\n",
    "\n",
    "            tfv.extractall('Val')\n",
    "\n",
    "            tfv.close()\n",
    "\n",
    "        \"\"\"Extract all .Tar Files in Training Data\"\"\"\n",
    "\n",
    "        for i in range(len(os.listdir('Train'))):\n",
    "\n",
    "            if '.tar' in os.listdir('Train')[i]:\n",
    "\n",
    "                tftn = tarfile.open(f\"Train/{os.listdir('Train')[i]}\")\n",
    "\n",
    "                tftn.extractall(f'Train/{i}')\n",
    "\n",
    "                tftn.close()\n",
    "\n",
    "                print(i, len(os.listdir(f'Train/{i}')))\n",
    "\n",
    "\n",
    "        \"\"\"Remove all .tar files from directory\"\"\"\n",
    "\n",
    "\n",
    "        for i in os.listdir('Train'):\n",
    "\n",
    "            if '.tar' in i: \n",
    "\n",
    "                os.remove(f'Train/{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8524824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1300\n",
      "1 1300\n",
      "2 1300\n",
      "3 1300\n",
      "4 1300\n",
      "5 1300\n",
      "6 1300\n",
      "7 1300\n",
      "8 1300\n",
      "9 1300\n",
      "10 1300\n",
      "11 1300\n",
      "12 1300\n",
      "13 755\n",
      "14 1300\n",
      "15 1300\n",
      "16 1300\n",
      "17 1207\n",
      "18 1300\n",
      "19 1300\n",
      "20 1300\n",
      "21 1053\n",
      "22 1055\n",
      "23 1300\n",
      "24 1300\n",
      "25 1300\n",
      "26 1300\n",
      "27 1300\n",
      "28 1300\n",
      "29 1300\n",
      "30 1300\n",
      "32 1300\n",
      "33 1300\n",
      "34 1071\n",
      "35 1300\n",
      "36 1300\n",
      "37 1300\n",
      "38 1300\n",
      "39 1300\n",
      "40 1300\n",
      "41 1300\n",
      "43 1300\n",
      "44 1300\n",
      "45 1300\n",
      "46 1053\n",
      "47 1300\n",
      "48 1300\n",
      "49 1300\n",
      "50 1300\n",
      "51 1300\n",
      "52 1300\n",
      "54 1300\n",
      "55 1300\n",
      "56 1300\n",
      "57 1300\n",
      "58 1300\n",
      "59 1300\n",
      "60 1300\n",
      "61 1300\n",
      "62 1300\n",
      "64 1300\n",
      "65 1300\n",
      "66 1300\n",
      "67 1300\n",
      "68 1300\n",
      "69 1300\n",
      "70 889\n",
      "71 889\n",
      "72 1300\n",
      "73 1300\n",
      "74 1300\n",
      "75 1300\n",
      "76 1300\n",
      "77 954\n",
      "78 1300\n",
      "79 1300\n",
      "80 1300\n",
      "81 1300\n",
      "83 1300\n",
      "84 1300\n",
      "85 1300\n",
      "86 1300\n",
      "88 1300\n",
      "89 1300\n",
      "90 1300\n",
      "91 1300\n",
      "92 1300\n",
      "93 1300\n",
      "94 1300\n",
      "95 1300\n",
      "96 1300\n",
      "98 1300\n",
      "99 1300\n",
      "101 1300\n",
      "102 1084\n",
      "103 1300\n",
      "104 1300\n",
      "106 1300\n",
      "107 1300\n",
      "108 1300\n",
      "109 1300\n",
      "110 1300\n",
      "111 1300\n",
      "112 1300\n",
      "113 1300\n",
      "114 1300\n",
      "115 1300\n",
      "116 1300\n",
      "117 1300\n",
      "118 1300\n",
      "119 1300\n",
      "120 1300\n",
      "121 1300\n",
      "122 1300\n",
      "123 1300\n",
      "125 1300\n",
      "127 1300\n",
      "128 1300\n",
      "129 1300\n",
      "130 1300\n",
      "131 1300\n",
      "132 1300\n",
      "133 1300\n",
      "134 1300\n",
      "135 1155\n",
      "136 1300\n",
      "138 1300\n",
      "139 1300\n",
      "140 1300\n",
      "141 1300\n",
      "142 1300\n",
      "144 1300\n",
      "145 1300\n",
      "146 1300\n",
      "147 1300\n",
      "148 1300\n",
      "149 1300\n",
      "150 1300\n",
      "151 1300\n",
      "152 1300\n",
      "153 1300\n",
      "154 1300\n",
      "155 1300\n",
      "156 1300\n",
      "157 1300\n",
      "158 1300\n",
      "159 1300\n",
      "160 1300\n",
      "161 1300\n",
      "162 1300\n",
      "163 1300\n",
      "164 1300\n",
      "165 1300\n",
      "166 1300\n",
      "167 1300\n",
      "168 1300\n",
      "169 1300\n",
      "170 1300\n",
      "172 1300\n",
      "173 1300\n",
      "174 1300\n",
      "175 1300\n",
      "176 1300\n",
      "177 1300\n",
      "178 1300\n",
      "179 1300\n",
      "180 1300\n",
      "181 1300\n",
      "182 1300\n",
      "183 1300\n",
      "184 1300\n",
      "185 1300\n",
      "186 1300\n",
      "187 1300\n",
      "188 1300\n",
      "190 1125\n",
      "191 1300\n",
      "192 1300\n",
      "193 1300\n",
      "194 1300\n",
      "195 1300\n",
      "196 1300\n",
      "197 1300\n",
      "198 1300\n",
      "199 1300\n",
      "201 1300\n",
      "202 1300\n",
      "204 1300\n",
      "205 1300\n",
      "206 1300\n",
      "207 1300\n",
      "208 1300\n",
      "210 1300\n",
      "211 1300\n",
      "212 1300\n",
      "213 1300\n",
      "214 1300\n",
      "215 1300\n",
      "216 1300\n",
      "217 1300\n",
      "218 1300\n",
      "219 1300\n",
      "220 1300\n",
      "221 1300\n",
      "222 1300\n",
      "223 1300\n",
      "224 1300\n",
      "225 1300\n",
      "226 1300\n",
      "228 1300\n",
      "229 1300\n",
      "233 1300\n",
      "235 1300\n",
      "236 1300\n",
      "238 1300\n",
      "239 1300\n",
      "240 1300\n",
      "241 1300\n",
      "242 1300\n",
      "244 1300\n",
      "245 1300\n",
      "246 1300\n",
      "247 1300\n",
      "250 1300\n",
      "251 1300\n",
      "252 1300\n",
      "253 1300\n",
      "254 738\n",
      "256 1300\n",
      "257 1300\n",
      "258 1300\n",
      "259 1300\n",
      "260 1300\n",
      "261 1300\n",
      "262 1300\n",
      "264 1300\n",
      "265 1300\n",
      "266 1300\n",
      "268 1300\n",
      "269 1300\n",
      "270 1300\n",
      "271 1300\n",
      "272 1176\n",
      "274 1300\n",
      "275 1300\n",
      "276 1300\n",
      "277 1300\n",
      "278 1300\n",
      "280 1194\n",
      "281 1300\n",
      "282 1300\n",
      "283 1300\n",
      "284 1300\n",
      "285 1029\n",
      "286 1300\n",
      "287 1300\n",
      "288 1300\n",
      "289 1300\n",
      "290 1300\n",
      "291 1300\n",
      "292 1300\n",
      "293 1300\n",
      "294 1300\n",
      "295 1300\n",
      "296 1300\n",
      "297 1300\n",
      "298 1154\n",
      "299 1154\n",
      "300 1300\n",
      "301 1300\n",
      "302 1300\n",
      "303 1300\n",
      "304 1300\n",
      "305 1300\n",
      "306 1300\n",
      "308 980\n",
      "309 1300\n",
      "310 1300\n",
      "311 1300\n",
      "312 1300\n",
      "313 1300\n",
      "315 1300\n",
      "316 1300\n",
      "317 1300\n",
      "318 1300\n",
      "319 1300\n",
      "320 1300\n",
      "322 1300\n",
      "323 1300\n",
      "324 1300\n",
      "325 1300\n",
      "326 1300\n",
      "327 1199\n",
      "329 1300\n",
      "330 1300\n",
      "331 1300\n",
      "333 1300\n",
      "334 1300\n",
      "335 1300\n",
      "336 1300\n",
      "337 1300\n",
      "340 1120\n",
      "341 1300\n",
      "342 977\n",
      "344 1300\n",
      "345 1300\n",
      "347 1300\n",
      "349 1300\n",
      "350 1300\n",
      "351 1300\n",
      "352 1300\n",
      "355 1300\n",
      "356 1300\n",
      "357 1300\n",
      "358 1300\n",
      "359 1300\n",
      "360 1300\n",
      "361 1300\n",
      "362 1300\n",
      "363 1254\n",
      "364 1300\n",
      "366 1300\n",
      "367 1300\n",
      "368 1300\n",
      "369 1300\n",
      "370 1300\n",
      "371 1300\n",
      "372 1273\n",
      "373 1273\n",
      "374 1300\n",
      "375 1300\n",
      "376 1300\n",
      "377 1300\n",
      "378 1300\n",
      "379 1300\n",
      "380 1300\n",
      "381 1282\n",
      "382 1282\n",
      "383 1282\n",
      "384 1300\n",
      "385 1300\n",
      "386 1300\n",
      "387 1300\n",
      "388 1300\n",
      "389 1300\n",
      "390 1300\n",
      "391 1300\n",
      "392 1300\n",
      "393 1300\n",
      "394 1141\n",
      "395 1141\n",
      "396 1141\n",
      "397 754\n",
      "398 1300\n",
      "399 1300\n",
      "400 1300\n",
      "401 1300\n",
      "402 1300\n",
      "405 1160\n",
      "406 1300\n",
      "407 1300\n",
      "408 1300\n",
      "409 1300\n",
      "410 1300\n",
      "411 1300\n",
      "412 1300\n",
      "413 1300\n",
      "415 1300\n",
      "417 1300\n",
      "419 1300\n",
      "420 1300\n",
      "422 1300\n",
      "424 1300\n",
      "425 1300\n",
      "427 1300\n",
      "428 1300\n",
      "431 1300\n",
      "432 1300\n",
      "433 1300\n",
      "435 1300\n",
      "436 1300\n",
      "437 1300\n",
      "438 1300\n",
      "439 1300\n",
      "441 1300\n",
      "442 1300\n",
      "443 732\n",
      "444 1300\n",
      "445 1300\n",
      "446 1300\n",
      "447 969\n",
      "448 969\n",
      "450 1300\n",
      "451 1300\n",
      "452 1300\n",
      "453 1300\n",
      "454 1300\n",
      "456 1300\n",
      "458 1300\n",
      "459 1300\n",
      "460 1300\n",
      "461 1300\n",
      "462 1300\n",
      "463 1300\n",
      "464 1300\n",
      "465 1300\n",
      "467 1300\n",
      "468 1300\n",
      "469 1300\n",
      "470 1300\n",
      "471 1300\n",
      "472 1300\n",
      "476 1300\n",
      "477 1300\n",
      "478 1136\n",
      "479 1300\n",
      "480 1300\n",
      "482 1300\n",
      "483 1300\n",
      "484 1300\n",
      "485 1247\n",
      "486 1300\n",
      "487 1300\n",
      "488 1300\n",
      "489 1069\n",
      "490 1300\n",
      "491 1300\n",
      "492 1300\n",
      "493 1300\n",
      "494 1300\n",
      "495 1300\n",
      "496 1300\n",
      "498 1300\n",
      "499 1300\n",
      "500 1300\n",
      "501 1300\n",
      "503 1300\n",
      "504 1300\n",
      "505 1300\n",
      "506 1300\n",
      "507 1300\n",
      "508 1300\n",
      "509 1300\n",
      "510 1300\n",
      "511 1300\n",
      "513 1300\n",
      "514 1300\n",
      "515 1300\n",
      "516 1300\n",
      "517 1300\n",
      "518 1300\n",
      "519 1300\n",
      "520 1300\n",
      "521 1300\n",
      "522 1300\n",
      "523 1300\n",
      "524 1300\n",
      "525 1300\n",
      "526 1300\n",
      "527 1300\n",
      "528 1133\n",
      "530 1300\n",
      "531 1300\n",
      "532 1300\n",
      "533 1300\n",
      "535 1300\n",
      "536 1300\n",
      "537 1300\n",
      "538 1300\n",
      "544 1300\n",
      "545 1300\n",
      "546 1300\n",
      "547 1300\n",
      "548 1300\n",
      "550 1300\n",
      "551 1300\n",
      "552 1300\n",
      "553 1300\n",
      "554 1300\n",
      "556 772\n",
      "557 1300\n",
      "558 1300\n",
      "559 1300\n",
      "561 1300\n",
      "562 1300\n",
      "564 1300\n",
      "566 1300\n",
      "567 1300\n",
      "569 1300\n",
      "570 1300\n",
      "572 1300\n",
      "573 1292\n",
      "574 1292\n",
      "575 1300\n",
      "579 1300\n",
      "580 1300\n",
      "581 1300\n",
      "582 1156\n",
      "583 1156\n",
      "585 1300\n",
      "586 1300\n",
      "590 969\n",
      "591 1300\n",
      "592 1300\n",
      "593 1300\n",
      "594 1300\n",
      "596 1165\n",
      "597 1165\n",
      "598 1165\n",
      "599 1300\n",
      "600 1300\n",
      "601 1300\n",
      "602 1300\n",
      "603 1300\n",
      "604 1300\n",
      "605 1300\n",
      "607 908\n",
      "608 1300\n",
      "609 1300\n",
      "610 1300\n",
      "611 1300\n",
      "612 1300\n",
      "613 1300\n",
      "614 1300\n",
      "615 1300\n",
      "616 1300\n",
      "617 1300\n",
      "618 1300\n",
      "619 1300\n",
      "620 1300\n",
      "621 1300\n",
      "622 1300\n",
      "626 1300\n",
      "628 1300\n",
      "629 1300\n",
      "630 1300\n",
      "631 1300\n",
      "634 1300\n",
      "635 1300\n",
      "636 1300\n",
      "637 1300\n",
      "638 1300\n",
      "640 1300\n",
      "641 1300\n",
      "642 1300\n",
      "643 1300\n",
      "645 1300\n",
      "646 1300\n",
      "647 1300\n",
      "649 1300\n",
      "650 1300\n",
      "651 1300\n",
      "654 1300\n",
      "655 1300\n",
      "656 1300\n",
      "657 1034\n",
      "658 1034\n",
      "660 1180\n",
      "661 1180\n",
      "662 1300\n",
      "665 1300\n",
      "666 1300\n",
      "667 1300\n",
      "668 1258\n",
      "670 1300\n",
      "671 1300\n",
      "672 1300\n",
      "673 1300\n",
      "674 1300\n",
      "675 1300\n",
      "676 1300\n",
      "677 1300\n",
      "681 1300\n",
      "686 891\n",
      "691 1004\n",
      "692 1300\n",
      "693 1300\n",
      "694 1300\n",
      "695 1300\n",
      "696 1300\n",
      "697 1300\n",
      "698 1236\n",
      "699 1300\n",
      "700 1300\n",
      "702 1300\n",
      "703 1300\n",
      "704 1300\n",
      "707 1300\n",
      "708 1300\n",
      "709 1300\n",
      "710 1300\n",
      "712 1300\n",
      "713 1300\n",
      "715 1300\n",
      "716 1300\n",
      "717 1300\n",
      "718 1300\n",
      "719 1300\n",
      "722 1300\n",
      "723 1300\n",
      "724 1300\n",
      "725 1300\n",
      "726 1300\n",
      "727 1300\n",
      "728 1300\n",
      "729 1300\n",
      "730 1300\n",
      "731 1300\n",
      "734 1186\n",
      "735 1186\n",
      "736 1300\n",
      "737 1300\n",
      "739 1300\n",
      "740 1300\n",
      "741 1300\n",
      "742 1300\n",
      "743 1300\n",
      "744 1300\n",
      "745 1300\n",
      "746 1300\n",
      "750 1300\n",
      "751 1045\n",
      "753 1300\n",
      "754 1300\n",
      "755 1300\n",
      "758 1300\n",
      "759 1300\n",
      "760 1300\n",
      "763 1300\n",
      "764 1300\n",
      "765 1300\n",
      "767 1300\n",
      "768 1300\n",
      "769 1300\n",
      "770 1300\n",
      "771 1300\n",
      "772 1300\n",
      "773 1300\n",
      "774 1300\n",
      "775 1300\n",
      "776 1283\n",
      "777 1283\n",
      "778 1283\n",
      "779 1283\n",
      "780 1283\n",
      "782 1300\n",
      "783 1300\n",
      "784 1300\n",
      "785 1300\n",
      "786 1300\n",
      "787 1300\n",
      "788 1300\n",
      "789 1300\n",
      "790 1300\n",
      "791 1300\n",
      "792 1300\n",
      "793 1300\n",
      "794 1300\n",
      "795 1300\n",
      "798 1300\n",
      "799 1300\n",
      "801 1300\n",
      "802 1300\n",
      "803 1300\n",
      "804 1300\n",
      "805 1300\n",
      "806 1300\n",
      "807 1300\n",
      "808 1300\n",
      "811 1300\n",
      "813 1300\n",
      "814 1300\n",
      "815 1300\n",
      "816 1300\n",
      "818 1300\n",
      "819 1300\n",
      "820 1300\n",
      "822 1300\n",
      "823 1300\n",
      "824 1300\n",
      "825 1029\n",
      "827 1300\n",
      "829 1300\n",
      "830 1300\n",
      "831 1300\n",
      "832 1300\n",
      "833 1300\n",
      "835 1300\n",
      "836 1300\n",
      "837 1300\n",
      "838 1300\n",
      "839 1300\n",
      "840 1300\n",
      "842 1300\n",
      "843 1300\n",
      "847 1300\n",
      "848 1300\n",
      "849 1300\n",
      "850 1149\n",
      "851 1149\n",
      "852 1149\n",
      "853 1300\n",
      "854 1300\n",
      "855 1300\n",
      "856 1300\n",
      "858 1300\n",
      "859 1300\n",
      "860 1300\n",
      "861 1300\n",
      "862 1300\n",
      "863 1300\n",
      "864 1300\n",
      "865 1300\n",
      "866 1300\n",
      "867 1300\n",
      "868 1300\n",
      "869 1300\n",
      "870 1300\n",
      "871 1300\n",
      "872 1300\n",
      "873 1300\n",
      "874 1300\n",
      "875 1300\n",
      "876 1300\n",
      "877 1213\n",
      "878 1213\n",
      "879 1213\n",
      "881 1300\n",
      "882 1300\n",
      "883 1300\n",
      "885 1300\n",
      "886 1300\n",
      "888 1300\n",
      "889 1300\n",
      "890 1300\n",
      "891 1300\n",
      "892 1300\n",
      "893 1300\n",
      "894 1300\n",
      "895 1300\n",
      "896 1300\n",
      "897 1300\n",
      "898 1300\n",
      "899 1300\n",
      "901 931\n",
      "903 1300\n",
      "904 1187\n",
      "905 1187\n",
      "907 1300\n",
      "908 1300\n",
      "909 1300\n",
      "911 1300\n",
      "914 1300\n",
      "915 1300\n",
      "916 1300\n",
      "917 1300\n",
      "918 1285\n",
      "920 1300\n",
      "921 1300\n",
      "922 1300\n",
      "923 1300\n",
      "924 1157\n",
      "925 1157\n",
      "927 1300\n",
      "930 1300\n",
      "932 1300\n",
      "933 1300\n",
      "934 1300\n",
      "935 1300\n",
      "936 1300\n",
      "937 1300\n",
      "939 1300\n",
      "941 1300\n",
      "942 1300\n",
      "943 1300\n",
      "945 1117\n",
      "946 1117\n",
      "947 1300\n",
      "949 1300\n",
      "951 1300\n",
      "952 1300\n",
      "953 1300\n",
      "954 1300\n",
      "955 1300\n",
      "956 1300\n",
      "957 1300\n",
      "958 1300\n",
      "959 1300\n",
      "960 1300\n",
      "961 1300\n",
      "962 1300\n",
      "963 1300\n",
      "964 1300\n",
      "965 1300\n",
      "967 1300\n",
      "968 1300\n",
      "969 1300\n",
      "970 1300\n",
      "971 1300\n",
      "972 1300\n",
      "978 1300\n",
      "979 1300\n",
      "980 1300\n",
      "981 1300\n",
      "982 1300\n",
      "983 1300\n",
      "984 1300\n",
      "985 1300\n",
      "986 1300\n",
      "987 1300\n",
      "988 1300\n",
      "992 1300\n",
      "993 1300\n",
      "994 1300\n",
      "995 1300\n",
      "997 1067\n",
      "998 1067\n",
      "999 1067\n"
     ]
    }
   ],
   "source": [
    "extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dffb9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in os.listdir('Train'):\n",
    "    \n",
    "#     if os.path.isdir(f'Train/{i}'):\n",
    "        \n",
    "#         for j in os.listdir(f'Train/{i}'):\n",
    "            \n",
    "#             os.remove(f'Train/{i}/{j}')\n",
    "            \n",
    "#         os.rmdir(f'Train/{i}')\n",
    "            \n",
    "#     else:\n",
    "        \n",
    "#         os.remove(f'Train/{i}')\n",
    "            \n",
    "# os.rmdir('Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1 \n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "cudnn.deterministic = True\n",
    "\n",
    "torch.cuda.device_count()\n",
    "\n",
    "START_EPOCH = 0\n",
    "\n",
    "#Model \n",
    "\n",
    "ARCH = 'resnet18'\n",
    "EPOCHS = 20\n",
    "LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PRINT_FREQ = 50\n",
    "TRAIN_BATCH=128\n",
    "VAL_BATCH=128\n",
    "WORKERS=2\n",
    "TRAINDIR=\"Train/\"\n",
    "VALDIR=\"Val/\"\n",
    "\n",
    "device = torch.cuda.set_device('cuda:0')\n",
    "\n",
    "device = 'GPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd972f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use TensorBoard for nice model training viz\"\"\"\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, writer, amp = True):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if device is not None:\n",
    "            images = images.cuda(device, non_blocking=True)\n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda(device, non_blocking=True)\n",
    "\n",
    "        # compute output, Use Torch Autocast for Automated Mixed Precision\n",
    "        \n",
    "        if amp: \n",
    "            \n",
    "            with autocast(dtype=torch.float16): #device_type='cuda', \n",
    "                output = model(images)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "        \n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            progress.display(i)\n",
    "            \n",
    "#     writer.add_scalar(\"Loss/train\", losses, epoch)\n",
    "#     writer.add_scalar(\"acc1/train\", top1, epoch)\n",
    "#     writer.add_scalar(\"acc5/train\", top5, epoch)\n",
    "#     writer.add_scalar(\"Time/train\", batch_time, epoch)\n",
    "    \n",
    "    return [losses, top1, top5, batch_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab86ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, writer):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            if device is not None:\n",
    "                images = images.cuda(device, non_blocking=True)\n",
    "            if torch.cuda.is_available():\n",
    "                target = target.cuda(device, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            \n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % PRINT_FREQ == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "#     writer.add_scalar(\"Loss/val\", losses, epoch)\n",
    "#     writer.add_scalar(\"acc1/val\", top1, epoch)\n",
    "#     writer.add_scalar(\"acc5/val\", top5, epoch)\n",
    "#     writer.add_scalar(\"Time/val\", batch_time, epoch)\n",
    "    \n",
    "\n",
    "    return [top1.avg, losses, top1, top5,  batch_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027efac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab934e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = LR * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Standardize RGB channels\"\"\"\n",
    "\n",
    "imagenet_mean_RGB = [0.47889522, 0.47227842, 0.43047404]\n",
    "imagenet_std_RGB = [0.229, 0.224, 0.225]\n",
    "\n",
    "normalize = transforms.Normalize(mean=imagenet_mean_RGB, std= imagenet_std_RGB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 1000\n",
    "\n",
    "model = models.__dict__[ARCH]()\n",
    "\n",
    "inf = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(inf, NUM_CLASSES)\n",
    "\n",
    "model.cuda('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), LR,\n",
    "                                momentum=MOMENTUM,\n",
    "                                weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4, pad_if_needed = True), #ran into an error where the image was below the 32,32 image size \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean_RGB, imagenet_std_RGB),\n",
    "])\n",
    "\n",
    "TRAINDIR = 'Train/'\n",
    "print(TRAINDIR)\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    TRAINDIR, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ec49ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split Train and validate on Training Image folders because \n",
    "Validate datafolder did not come with lables\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "orig_set = torchvision.datasets.ImageFolder(TRAINDIR, transform=transform_train) \n",
    "\n",
    "n = len(orig_set)  # total number of examples\n",
    "\n",
    "print(n)\n",
    "\n",
    "sub = np.array(random.sample(range(n), int(n*0.1))) # Random Subset with seed = 1 for repeatability \n",
    "\n",
    "sub_trn = np.arange(n)\n",
    "sub_trn = sub_trn[~sub]\n",
    "\n",
    "print(len(sub_trn), len(sub))\n",
    "print(sub[69]) # prove that the numbers are the same across the two notebooks \n",
    "\n",
    "n_test = int(0.1 * n)  # take ~10% for test\n",
    "val_set = torch.utils.data.Subset(orig_set, sub)  # take first 10%\n",
    "train_set = torch.utils.data.Subset(orig_set, range(n_test, n))  # take the rest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c63d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=TRAIN_BATCH, shuffle=True,\n",
    "        num_workers=WORKERS, pin_memory=True, sampler=None)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_set, batch_size=VAL_BATCH, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True, sampler=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1043ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc1 = 0\n",
    "\n",
    "\"\"\"upgrade my version of pytorch to get usage of autocast\"\"\"\n",
    "\n",
    "!pip install --upgrade torch\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast \n",
    "\n",
    "resnet_18_single_node_scores_train = [] \n",
    "resnet_18_single_node_scores_val = [] \n",
    "\n",
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "#    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train_scores = train(train_loader, model, criterion, optimizer, epoch, writer)\n",
    "\n",
    "    print(type(train_scores[0]))\n",
    "    # evaluate on validation set\n",
    "    val_scores = validate(val_loader, model, criterion, writer)\n",
    "    acc1 = val_scores[0]\n",
    "    \n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': ARCH,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "    \n",
    "    scheduler.step()\n",
    "    print('lr: ' + str(scheduler.get_last_lr()))\n",
    "    \n",
    "    resnet_18_single_node_scores_train.append(train_scores)\n",
    "    resnet_18_single_node_scores_val.append(val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67076657",
   "metadata": {},
   "source": [
    "# Okay we have run several models and have collected the data on their performance differences\n",
    "\n",
    "# Lets move to Distributed Multi Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dffe238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "GPUs on device:  1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Import Necessary Libraries\"\"\"\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "\"\"\"Check Backends and GPUs\"\"\"\n",
    "\n",
    "print(torch.distributed.is_available())\n",
    "\n",
    "print(torch.distributed.is_gloo_available())\n",
    "\n",
    "print(torch.distributed.is_nccl_available())\n",
    "\n",
    "print(torch.distributed.is_mpi_available())\n",
    "\n",
    "print('GPUs on device: ', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ef097",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_process_group(\"gloo\", rank=0, world_size=2, init_method='tcp://172.31.87.52:12355')\n",
    "print('process groups initialized') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db261e",
   "metadata": {},
   "source": [
    "# This is where the process hangs and will not continue. I followed along many Pytorch tutorials, opened all ports on my docker containers, and opened my security groups for all inbound IP traffic from internal IP addresses of my AWS ec2 instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Modify Data Loaders for Distributed Sampling\"\"\"\n",
    "\n",
    "def Distrib_sample(train_loader, val_loader, TRAIN_BATCH):\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            train_set, batch_size=TRAIN_BATCH, shuffle=True,\n",
    "            num_workers=WORKERS, pin_memory=True, sampler=DistributedSampler(train_set))\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "            val_set, batch_size=TRAIN_BATCH, shuffle=True,\n",
    "            num_workers=WORKERS, pin_memory=True, sampler=DistributedSampler(val_set))\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(ARCH, EPOCHS, BATCH, train_loader, val_loader, rank): \n",
    "    \n",
    "    #Initiate DDP \n",
    "    setup(rank, 2)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    \"\"\"Training Function\"\"\"\n",
    "    LR = 0.1\n",
    "    MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    PRINT_FREQ = 50\n",
    "    TRAIN_BATCH=BATCH\n",
    "    VAL_BATCH= BATCH\n",
    "    WORKERS=2\n",
    "\n",
    "\n",
    "    #Create Model with Imagenet output Layer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), LR,\n",
    "                                momentum=MOMENTUM,\n",
    "                                weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    model = models.__dict__[ARCH]()\n",
    "    inf = model.fc.in_features\n",
    "    model.fc = nn.Linear(inf, NUM_CLASSES)\n",
    "    model.cuda('cuda')\n",
    "    model = DDP(model, device_ids = [0])\n",
    "\n",
    "    #Instantiate Metrics Lists \n",
    "    train_metrics = [] \n",
    "    val_metrics = [] \n",
    "    \n",
    "    #wrap Data Loaders in Distributed Sampler \n",
    "    train_loader_val_loader = Distrib_sample(train_loader, val_loader, TRAIN_BATCH)\n",
    "    \n",
    "    for epoch in range(START_EPOCH, EPOCHS):\n",
    "        \n",
    "        #Modify Learning Rate (Cosine Annealing)\n",
    "        #adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train_scores = train(train_loader, model, criterion, optimizer, epoch, writer, amp = True)\n",
    "\n",
    "        print(type(train_scores[0]))\n",
    "        # evaluate on validation set\n",
    "        val_scores = validate(val_loader, model, criterion, writer)\n",
    "        acc1 = val_scores[0]\n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': ARCH,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "\n",
    "        scheduler.step()\n",
    "        print('lr: ' + str(scheduler.get_last_lr()))\n",
    "\n",
    "        #record performance\n",
    "        prof.step()\n",
    "        train_metrics.append(train_scores)\n",
    "        val_metrics.append(val_scores)\n",
    "\n",
    "    return train_metrics, val_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
